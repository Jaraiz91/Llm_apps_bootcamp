{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Documentation\n",
    "* [LangGraph Documentation](https://python.langchain.com/docs/langgraph/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* Framework to develop Multi-Agent LLM Apps.\n",
    "* Python and Javascript versions.\n",
    "* Main uses:\n",
    "    * add cycles to LLM App.\n",
    "    * add persistence to LLM App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "* Graph: agent, multi-agent.\n",
    "* Nodes: actions.\n",
    "* Edges: node connections.\n",
    "* State: memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we initialize our model and an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0)\n",
    "\n",
    "agent = MessageGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we add a single node to the agent, called \"node1\", which simply calls the LLM with the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.message.MessageGraph at 0x1bb5a6b5150>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.add_node(\"node1\", llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We add an edge from this \"node1\" node to the special string END (`__end__`). This means that execution will end after the current node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.message.MessageGraph at 0x1bb5a6b5150>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.add_edge(\"node1\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We set \"node1\" as the entry point to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.message.MessageGraph at 0x1bb5a6b5150>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.set_entry_point(\"node1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We compile the agent, translating it to low-level pregel operations ensuring that it can be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_agent = agent.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can run the agent now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 1 + 1?', additional_kwargs={}, response_metadata={}, id='66512616-6f68-40b8-a751-ac7035e0198d'),\n",
       " AIMessage(content='1 + 1 equals 2.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 15, 'total_tokens': 24, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4fbe98c1-d424-4db7-8cdc-4ee607e4b4ed-0', usage_metadata={'input_tokens': 15, 'output_tokens': 9, 'total_tokens': 24})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_agent.invoke(HumanMessage(\"What is 1 + 1?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we execute the agent:\n",
    "* LangGraph adds the input message to the  state, then passes the state to the entrypoint node, \"node1\".\n",
    "* The \"node1\" node executes, invoking the chat model.\n",
    "* The chat model returns an AIMessage. LangGraph adds this to the state.\n",
    "* Execution progresses to the special END value and outputs the final state.\n",
    "* And as a result, we get a list of two chat messages as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent with a router and conditional edges\n",
    "* Let's allow the LLM to conditionally call a \"multiply\" node using tool calling.\n",
    "* We'll recreate our previous agent with an additional \"multiply\" tool that will take the result of the most recent message, if it is a tool call, and calculate the result.\n",
    "* We will bind the multiply tool to the OpenAI model to allow the llm to optionally use the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "@tool\n",
    "def multiply(first_number: int, second_number: int):\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return first_number * second_number\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "llm_with_tools = llm.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.message.MessageGraph at 0x1bb725af280>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_conditional_edges = MessageGraph()\n",
    "agent_with_conditional_edges.add_node(\"node1\", llm_with_tools)\n",
    "agent_with_conditional_edges.set_entry_point(\"node1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the second node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.message.MessageGraph at 0x1bb725af280>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_node = ToolNode([multiply])\n",
    "agent_with_conditional_edges.add_node(\"multiply\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the edge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.message.MessageGraph at 0x1bb725af280>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_conditional_edges.add_edge(\"multiply\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using conditional edges, which call a function on the current state and routes execution to a node the function's output:\n",
    "* If the \"node1\" node returns a message expecting a tool call, we want to execute the \"multiply\" node.\n",
    "* If not, we can just end execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.message.MessageGraph at 0x1bb725af280>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "def router(state: List[BaseMessage]) -> Literal['multiply', '__end__']:\n",
    "    tool_calls = state[-1].additional_kwargs.get('tool_calls', [])\n",
    "    if len(tool_calls):\n",
    "        return 'multiply'\n",
    "    else:\n",
    "        return '__end__'\n",
    "    \n",
    "agent_with_conditional_edges.add_conditional_edges('node1', router)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_agent_with_conditional_edges = agent_with_conditional_edges.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 123 * 456?', additional_kwargs={}, response_metadata={}, id='f6bf203d-4b77-4e80-8666-fc2dbe601f34'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Acj5HMfTxE2BreUFPDHDTe1q', 'function': {'arguments': '{\"first_number\": 123, \"second_number\": 456}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 57, 'total_tokens': 92, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c541656f-e415-4e97-8b17-c53395c6cf36-0', tool_calls=[{'name': 'multiply', 'args': {'first_number': 123, 'second_number': 456}, 'id': 'call_Acj5HMfTxE2BreUFPDHDTe1q', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 35, 'total_tokens': 92}),\n",
       " ToolMessage(content='56088', name='multiply', id='b9bcf387-9ad1-47a1-b05c-4ce00bd01f64', tool_call_id='call_Acj5HMfTxE2BreUFPDHDTe1q')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_agent_with_conditional_edges.invoke(HumanMessage(\"What is 123 * 456?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While conversational responses are outputted directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={}, id='812ec503-2f0c-4629-8b6a-322d3f56a3b1'),\n",
       " AIMessage(content='My name is Assistant. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 54, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3862a95b-6326-47b3-8f3a-045e454422dd-0', usage_metadata={'input_tokens': 54, 'output_tokens': 14, 'total_tokens': 68})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_agent_with_conditional_edges.invoke(HumanMessage('What is your name?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent with cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will use Tavily as online search tool:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "online_search_tool = [TavilySearchResults(max_results=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can now wrap these tools in a simple LangGraph ToolNode. This class receives the list of messages (containing tool_calls), calls the tool(s) the LLM has requested to run, and returns the output as new ToolMessage(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node_with_online_search = ToolNode(online_search_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for OpenAI tool calling using the bind_tools() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0, streaming=True)\n",
    "llm_with_online_search_tool = llm.bind_tools(online_search_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's now define the state of the agent.\n",
    "* Each node will return operations to update the state.\n",
    "* For this example, we want each node to just add messages to the state. Therefore, in this case the state of the agent will be a list of messages. In other projects, the state can be any type.\n",
    "* We will use a TypedDict with one key (messages) and annotate it so that we always add to the messages key when updating it using the `is always added to` with the second parameter (operator.add)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "\n",
    "def add_messages(left:list, right:list):\n",
    "    \"\"\"Add-don´t overwrite.\"\"\"\n",
    "    return left + right\n",
    "\n",
    "class AgenState(TypedDict):\n",
    "    #The 'add_messages' function within the annotation defines\n",
    "    # *how* updates should be merged into the state\n",
    "    messages: Annotated[list, add_messages]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's now define the nodes of this agent:\n",
    "    * the node responsible for deciding what (if any) actions to take.\n",
    "    * if the previous node decides to take an action, this second node will then execute that action calling the online search tool.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will also define the edges that will interconnect the nodes.\n",
    "* One will be a Conditional Edge. After the node1 is called, the llm will dedice either:\n",
    "    * a. Run the online search tool (node2), OR\n",
    "    * b. Finish\n",
    "\n",
    "* The second will be a normal Edge: after the online search tool (node2) is invoked, the graph should always return to the node1 to decide what to do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def should_continue(state: AgenState) -> Literal['node2', '__end__']:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    #If the LLM makes a tool call, then we route to the action node\n",
    "    if last_message.tool_calls:\n",
    "        return 'node2'\n",
    "    else:\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "        return '__end__'\n",
    "    \n",
    "#Define the function that calls the model\n",
    "def call_model(state: AgenState):\n",
    "    messages = state['messages']\n",
    "    response = llm_with_online_search_tool.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1bb745d3640>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "# Define a new graph (agent)\n",
    "workflow = StateGraph(AgenState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"node1\", call_model)\n",
    "workflow.add_node(\"node2\", tool_node_with_online_search)\n",
    "\n",
    "# Set the entrypoint as 'node1\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point('node1')\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `node1`.\n",
    "    # This means these are the edges taken after the `node1` node is called.\n",
    "    \"node1\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `online_search_tool` to `node1`.\n",
    "# This means that after `online_search_tool` is called, `node1` node is called next.\n",
    "workflow.add_edge('node2', 'node1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_cycles = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_SrFwhyyQESTI5CGmlm9hoTaB', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-d1bb0371-9f4e-41f9-935f-763145368862-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_SrFwhyyQESTI5CGmlm9hoTaB', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1735645505, \\'localtime\\': \\'2024-12-31 03:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1735645500, \\'last_updated\\': \\'2024-12-31 03:45\\', \\'temp_c\\': 6.1, \\'temp_f\\': 43.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Clear\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/113.png\\', \\'code\\': 1000}, \\'wind_mph\\': 4.3, \\'wind_kph\\': 6.8, \\'wind_degree\\': 19, \\'wind_dir\\': \\'NNE\\', \\'pressure_mb\\': 1023.0, \\'pressure_in\\': 30.2, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 4.7, \\'feelslike_f\\': 40.5, \\'windchill_c\\': 7.9, \\'windchill_f\\': 46.2, \\'heatindex_c\\': 8.1, \\'heatindex_f\\': 46.5, \\'dewpoint_c\\': 6.1, \\'dewpoint_f\\': 43.0, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 7.2, \\'gust_kph\\': 11.7}}\"}]', name='tavily_search_results_json', tool_call_id='call_SrFwhyyQESTI5CGmlm9hoTaB', artifact={'query': 'weather in San Francisco', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Weather in San Francisco', 'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1735645505, 'localtime': '2024-12-31 03:45'}, 'current': {'last_updated_epoch': 1735645500, 'last_updated': '2024-12-31 03:45', 'temp_c': 6.1, 'temp_f': 43.0, 'is_day': 0, 'condition': {'text': 'Clear', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 4.3, 'wind_kph': 6.8, 'wind_degree': 19, 'wind_dir': 'NNE', 'pressure_mb': 1023.0, 'pressure_in': 30.2, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 0, 'feelslike_c': 4.7, 'feelslike_f': 40.5, 'windchill_c': 7.9, 'windchill_f': 46.2, 'heatindex_c': 8.1, 'heatindex_f': 46.5, 'dewpoint_c': 6.1, 'dewpoint_f': 43.0, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 7.2, 'gust_kph': 11.7}}\", 'score': 0.9151957, 'raw_content': None}], 'response_time': 3.43}),\n",
       "  AIMessage(content='The current weather in San Francisco is as follows:\\n- Temperature: 43.0°F (6.1°C)\\n- Condition: Clear\\n- Wind: 4.3 mph (6.8 kph) from NNE\\n- Humidity: 86%\\n- Pressure: 30.2 in\\n- Visibility: 9.0 miles\\n\\nFor more details, you can visit [Weather API](https://www.weatherapi.com/).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-eb483316-bcd5-4bae-833a-361b2f33494b-0')]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "agent_with_cycles.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional documentation\n",
    "* [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/).\n",
    "* [LangGraph How-To Guides](https://langchain-ai.github.io/langgraph/how-tos/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
