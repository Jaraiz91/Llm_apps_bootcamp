{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chain\n",
    "\n",
    "* Perform several actions in a particular order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chatModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell me a curious fact about \u001b[39m\u001b[38;5;132;01m{politician}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m \u001b[43mchatModel\u001b[49m \u001b[38;5;241m|\u001b[39m StrOutputParser()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chatModel' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {politician}\")\n",
    "\n",
    "chain = prompt | chatModel | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does StrOutputParser do?**\n",
    "\n",
    "* The StrOutputParser is a specific tool within LangChain that simplifies the output from these language models. It takes the complex or structured output from the models and converts it into plain text (a string). This makes it easier to use this output in applications, like displaying it to users or processing it further for other purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about JFK is that he was the youngest person to ever be elected President of the United States at the age of 43.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"politician\": \"jfk\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable execution order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../data/png/lcel-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 ways to execute chains: Invoke, Stream, Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".invoke() call the chain on an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why did the woke person go to therapy? Because they couldn't handle all the triggers!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 13, 'total_tokens': 31}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-03fff325-7737-4317-ae06-2c45f770d652-0', usage_metadata={'input_tokens': 13, 'output_tokens': 18, 'total_tokens': 31})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"woke\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".stream() call the chain on an input and stream back chunks of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the woke person go to therapy? Because they couldn't handle the reality of their own privilege!"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\": \"woke\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop is used to show responses piece by piece as they are received and prints out responses immediately.\n",
    "\n",
    "* `end=\"\"`: This parameter ensures that each piece of content is added without adding a new line.\n",
    "* `flush=True`: This parameter forces the output buffer to be flushed immediately after each print statement, ensuring that each piece of content is displayed to the user as soon as it is received witohut any delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".batch() is called when there is a list of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Why did the cheese go to the doctor? Because it was feeling blue!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0ce5b969-b04b-4abe-b62d-179e12694ebc-0', usage_metadata={'input_tokens': 13, 'output_tokens': 15, 'total_tokens': 28}),\n",
       " AIMessage(content='Why did Wednesday go to therapy? Because it had a case of the mid-week blues!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 14, 'total_tokens': 32}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1fe05af3-f5f7-49cd-8fee-9573623c8622-0', usage_metadata={'input_tokens': 14, 'output_tokens': 18, 'total_tokens': 32})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\":\"cheese\"}, {\"topic\": \"wednesday\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RunnablePassthrough.\n",
    "* RunnableLambda.\n",
    "* RunnableParallel\n",
    "    * itemgetter\n",
    "* RunnableBranch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough\n",
    "   \n",
    "   * It does not do anything to the input data\n",
    "   * will output the original input without any modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abram'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = RunnablePassthrough()\n",
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda\n",
    "\n",
    " * To use a custom function inside a LCEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname(name:str) -> str:\n",
    "    return f\"{name}ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abramovich'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = RunnablePassthrough() | RunnableLambda(russian_lastname)\n",
    "\n",
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel\n",
    "  \n",
    "  * For running tasks in parallel\n",
    "  * Probably the most important and useful runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'Abram', 'operation_b': 'Abramovich'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "\n",
    "\n",
    "        \n",
    "        \"operation_b\": RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")\n",
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': {'name1': 'Jordan', 'name': 'Abram'},\n",
       " 'operation_b': 'Abramovich'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"operation_b\": lambda x : x[\"name\"] + \"ovich\"\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke({\n",
    "    \"name1\": \"Jordan\",\n",
    "    \"name\": \"Abram\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add more runnables to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}, also tell me who is {operation_a}\")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname_from_dictioanry(person):\n",
    "    return person['name'] + \"ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": RunnableLambda(russian_lastname_from_dictioanry),\n",
    "        \"operation_c\": RunnablePassthrough()\n",
    "    }\n",
    ") | prompt | Model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A curious fact about Roman Abramovich is that he is known for his extravagant collection of luxury yachts, including the Eclipse which is one of the largest and most expensive private yachts in the world.\\n\\n{'name1': 'Jordan', 'name': 'Abram'} are two separate people. Jordan is a common first name, while Abram is a less common first name. There is no specific information given about who these individuals are, so it is unclear if they are related in any way.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"name1\": \"Jordan\",\n",
    "    \"name\": \"Abram\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI Accelera's alumni are individuals from all continents and top companies, totaling more than 10,000.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 10,000 alumni from all continents and top companies\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"who are the alumni of AI Accelera\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important: the syntax of RunnableParallel can have several variations.\n",
    "* When composing a RunnableParallel with another Runnable you do not need to wrap it up in the RunnableParallel class. Inside a chain, the next three syntaxs are equivalent:\n",
    "    * `RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})`\n",
    "    * `RunnableParallel(context=retriever, question=RunnablePassthrough())`\n",
    "    * `{\"context\": retriever, \"question\": RunnablePassthrough()}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using itemgetter with RunnableParallel\n",
    "* When you are calling the LLM with several different input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI Accelera has trained more than 5,000 Enterprise Alumni, arrr!'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 5,000 Enterprise Alumni.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = ( \n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\")\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"How many enterprise Alumni has trained AI Accelera?\", \"language\": \"Pirate English\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableBranch: Router Chain\n",
    "* A RunnableBranch is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input.\n",
    "* **A RunnableBranch is initialized with a list of (condition, runnable) pairs and a default runnable**. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n",
    "* For advanced uses, a [custom function](https://python.langchain.com/v0.1/docs/expression_language/how_to/routing/) may be a better alternative than RunnableBranch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following advanced example can classify and respond to user questions based on specific topics like rock, politics, history, sports, or general inquiries. **It uses some new topics that we will explain in the following lesson**. Here’s a simplified explanation of each part:\n",
    "\n",
    "1. **Prompt Templates**: Each template is tailored for a specific topic:\n",
    "   - **rock_template**: Configured for rock and roll related questions.\n",
    "   - **politics_template**: Tailored to answer questions about politics.\n",
    "   - **history_template**: Designed for queries related to history.\n",
    "   - **sports_template**: Set up to address sports-related questions.\n",
    "   - **general_prompt**: A general template for queries that don't fit the specific categories.\n",
    "\n",
    "   Each template includes a placeholder `{input}` where the actual user question will be inserted.\n",
    "\n",
    "2. **RunnableBranch**: This is a branching mechanism that selects which template to use based on the topic of the question. It evaluates conditions (like `x[\"topic\"] == \"rock\"`) to determine the topic and uses the appropriate prompt template.\n",
    "\n",
    "3. **Topic Classifier**: A Pydantic class that classifies the topic of a user's question into one of the predefined categories (rock, politics, history, sports, or general).\n",
    "\n",
    "4. **Classifier Chain**:\n",
    "   - **Chain**: Processes the user's input to predict the topic.\n",
    "   - **Parser**: Extracts the predicted topic from the classifier's output.\n",
    "\n",
    "5. **RunnablePassthrough**: This component feeds the user's input and the classified topic into the RunnableBranch.\n",
    "\n",
    "6. **Final Chain**:\n",
    "   - The user's input is first processed to classify its topic.\n",
    "   - The appropriate prompt is then selected based on the classified topic.\n",
    "   - The selected prompt is used to formulate a question which is then sent to a model (like ChatOpenAI).\n",
    "   - The model’s response is parsed as a string and returned.\n",
    "\n",
    "7. **Execution**:\n",
    "   - The chain is invoked with a sample question, \"Who was Napoleon Bonaparte?\" \n",
    "   - Based on the classification, it selects the appropriate template, generates a query to the chat model, and processes the response.\n",
    "\n",
    "The system effectively creates a dynamic response generator that adjusts the way it answers based on the topic of the inquiry, making use of specialized knowledge for different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "rock_template = \"\"\"You are a very smart rock and roll professor. \\\n",
    "You are great at answering questions about rock and roll in a concise\\\n",
    "and easy to understand manner.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "rock_prompt = PromptTemplate.from_template(rock_template)\n",
    "\n",
    "politics_template = \"\"\"You are a very good politics professor. \\\n",
    "You are great at answering politics questions..\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "politics_prompt = PromptTemplate.from_template(politics_template)\n",
    "\n",
    "history_template = \"\"\"You are a very good history teacher. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_prompt = PromptTemplate.from_template(history_template)\n",
    "\n",
    "sports_template = \"\"\" You are a sports teacher.\\\n",
    "You are great at answering sports questions.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "sports_prompt = PromptTemplate.from_template(sports_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}\"\n",
    ")\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"rock\", rock_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"politics\", politics_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"history\", history_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"sports\", sports_prompt),\n",
    "  general_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"Classify the topic of the user question\"\n",
    "    \n",
    "    topic: Literal[\"rock\", \"politics\", \"history\", \"sports\"]\n",
    "    \"The topic of the user question. One of 'rock', 'politics', 'history', 'sports' or 'general'.\"\n",
    "\n",
    "classifier_function = convert_to_openai_function(TopicClassifier)\n",
    "\n",
    "llm = ChatOpenAI().bind(functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}) \n",
    "\n",
    "parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "\n",
    "classifier_chain = llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `classifier_function` classifies or categorizes the topic of a user's question into specific categories such as \"rock,\" \"politics,\" \"history,\" or \"sports.\" Here’s how it works in simple terms:\n",
    "\n",
    "1. **Conversion to Function**: It converts the `TopicClassifier` Pydantic class, which is a predefined classification system, into a function that can be easily used with LangChain. This conversion process involves wrapping the class so that it can be integrated and executed within an OpenAI model.\n",
    "\n",
    "2. **Topic Detection**: When you input a question, this function analyzes the content of the question to determine which category or topic it belongs to. It looks for keywords or patterns that match specific topics. For example, if the question is about a rock band, the classifier would identify the topic as \"rock.\"\n",
    "\n",
    "3. **Output**: The function outputs the identified topic as a simple label, like \"rock\" or \"history.\" This label is then used by other parts of the LangChain to decide how to handle the question, such as choosing the right template for formulating a response.\n",
    "\n",
    "In essence, the `classifier_function` acts as a smart filter that helps the system understand what kind of question is being asked so that it can respond more accurately and relevantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Napoleon Bonaparte was a French military and political leader who rose to prominence during the French Revolution. He eventually crowned himself Emperor of the French in 1804 and ruled over much of Europe during the early 19th century. He is known for his military conquests, administrative reforms, and lasting impact on European politics and society. Napoleon's reign came to an end with his defeat at the Battle of Waterloo in 1815, after which he was exiled to the island of Saint Helena, where he died in 1821.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "final_chain = (\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"input\") | classifier_chain) \n",
    "    | prompt_branch \n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_chain.invoke(\n",
    "    {\"input\": \"Who was Napoleon Bonaparte?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in functions for Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of .bind() to add arguments to a Runnable in a LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me a curious fact about {soccer_player}\")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about Cristiano '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model.bind(stop=[\"Ronaldo\"]) | output_parser\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_input': 'whatever'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain =RunnableParallel({'original_input': RunnablePassthrough()})\n",
    "chain.invoke(\"whatever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_input': 'whatever', 'uppercase': 'WHATEVER'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_uppercase(arg):\n",
    "    return arg['original_input'].upper()\n",
    "chain =RunnableParallel({'original_input': RunnablePassthrough()}).assign(uppercase=RunnableLambda(make_uppercase))\n",
    "chain.invoke(\"whatever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinin LCEL Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chain: LLM Model + prompt + output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Napoleon Bonaparte was a French military leader and emperor who rose to power during the French Revolution.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"write a brief sentence about {topic}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"topic\": \"napoleon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coercion: a chain inside a chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remember: almost any component in LangChain (prompts, models, output_parser, etc) can be used as a Runnable\n",
    "* **Runnables can be chained together using thw pipe operator | . The resulting chains of runnables are also Runnables themselves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Opinions on Napoleon Bonaparte's impact on humanity vary. Some argue that his military conquests and political reforms had a lasting impact on Europe and contributed to the spread of revolutionary ideals. Others criticize his authoritarian rule and the wars he waged, which caused immense suffering and loss of life. Ultimately, whether Napoleon's legacy is seen as positive or negative for humanity depends on one's perspective and interpretation of history.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historian_prompt = ChatPromptTemplate.from_template(\"Was {topic} positive for humanity?\")\n",
    "composed_chain = {'topic': chain} | historian_prompt | model | output_parser\n",
    "composed_chain.invoke({\"topic\": \"napoleon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, Attila was not a positive figure for humanity. He was known for his brutal military campaigns and ruthless tactics, causing widespread destruction and terror throughout Europe. His invasion of various territories led to the death and suffering of countless people, making him a feared and despised figure in history.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain.invoke({'topic': 'Attila'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Le pays dont François Mitterrand était originaire, la France, se trouve sur le continent européen.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the country {politician} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"what continent is the  country {country} in? respond in {language}\")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain1 = prompt1 | model | output_parser\n",
    "chain2 = (\n",
    "    {\"country\": chain1, \"language\": itemgetter('language')}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "chain2.invoke({\"politician\": \"Miterrand\", \"language\": \"french\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
