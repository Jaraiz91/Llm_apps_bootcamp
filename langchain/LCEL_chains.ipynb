{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chain\n",
    "\n",
    "* Perform several actions in a particular order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {politician}\")\n",
    "\n",
    "chain = prompt | chatModel | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does StrOutputParser do?**\n",
    "\n",
    "* The StrOutputParser is a specific tool within LangChain that simplifies the output from these language models. It takes the complex or structured output from the models and converts it into plain text (a string). This makes it easier to use this output in applications, like displaying it to users or processing it further for other purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A curious fact about JFK is that he was a collector of maritime artifacts and spent much of his free time sailing on his family's boat, the Victura. He was also known to have a fascination with naval history and often visited maritime museums.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"politician\": \"jfk\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable execution order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../data/png/lcel-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 ways to execute chains: Invoke, Stream, Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".invoke() call the chain on an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the woke person bring a ladder to the protest? Because they wanted to take their activism to a higher level!', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 13, 'total_tokens': 37}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-770f34e4-d1a3-4064-8480-d8cdef857e84-0', usage_metadata={'input_tokens': 13, 'output_tokens': 24, 'total_tokens': 37})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"woke\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".stream() call the chain on an input and stream back chunks of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the woke activist bring a pillow to the debate? Because they wanted to make sure everyone was staying on topic!"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\", \"woke\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop is used to show responses piece by piece as they are received and prints out responses immediately.\n",
    "\n",
    "* `end=\"\"`: This parameter ensures that each piece of content is added without adding a new line.\n",
    "* `flush=True`: This parameter forces the output buffer to be flushed immediately after each print statement, ensuring that each piece of content is displayed to the user as soon as it is received witohut any delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".batch() is called when there is a list of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Why did the cheese go to the art exhibit? Because it wanted to see the \"brie\"lent artwork!', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 13, 'total_tokens': 37}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4ee5bace-58c0-45f3-addc-1d65ef7aab41-0', usage_metadata={'input_tokens': 13, 'output_tokens': 24, 'total_tokens': 37}),\n",
       " AIMessage(content='Why did Wednesday go to the therapist? Because it had a case of the mid-week blues!', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 14, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-63f4c235-8413-43ec-a089-eaeeffda50eb-0', usage_metadata={'input_tokens': 14, 'output_tokens': 19, 'total_tokens': 33})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\":\"cheese\"}, {\"topic\": \"wednesday\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RunnablePassthrough.\n",
    "* RunnableLambda.\n",
    "* RunnableParallel\n",
    "    * itemgetter\n",
    "* RunnableBranch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough\n",
    "   \n",
    "   * It does not do anything to the input data\n",
    "   * will output the original input without any modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abram'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = RunnablePassthrough()\n",
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda\n",
    "\n",
    " * To use a custom function inside a LCEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname(name:str) -> str:\n",
    "    return f\"{name}ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abramovich'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = RunnablePassthrough() | RunnableLambda(russian_lastname)\n",
    "\n",
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel\n",
    "  \n",
    "  * For running tasks in parallel\n",
    "  * Probably the most important and useful runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'Abram', 'operation_b': 'Abramovich'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"operation_b\": RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")\n",
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': {'name1': 'Jordan', 'name': 'Abram'},\n",
       " 'operation_b': 'Abramovich'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"operation_b\": lambda x : x[\"name\"] + \"ovich\"\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke({\n",
    "    \"name1\": \"Jordan\",\n",
    "    \"name\": \"Abram\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add more runnables to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname_from_dictioanry(person):\n",
    "    return person['name'] + \"ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": RunnableLambda(russian_lastname_from_dictioanry),\n",
    "        \"operation_c\": RunnablePassthrough()\n",
    "    }\n",
    ") | prompt | Model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One curious fact about Roman Abramovich is that he once owned the world's largest yacht, the Eclipse. The yacht is equipped with two helicopter pads, a submarine, and a missile defense system, making it one of the most luxurious and secure vessels in the world. Abramovich reportedly spent over $1.5 billion on the yacht, showcasing his extravagant lifestyle and wealth.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"name1\": \"Jordan\",\n",
    "    \"name\": \"Abram\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The alumni of AI Accelera are individuals from all continents and top companies.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 10,000 alumni from all continents and top companies\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"who are the alumni of AI Accelera\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important: the syntax of RunnableParallel can have several variations.\n",
    "* When composing a RunnableParallel with another Runnable you do not need to wrap it up in the RunnableParallel class. Inside a chain, the next three syntaxs are equivalent:\n",
    "    * `RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})`\n",
    "    * `RunnableParallel(context=retriever, question=RunnablePassthrough())`\n",
    "    * `{\"context\": retriever, \"question\": RunnablePassthrough()}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using itemgetter with RunnableParallel\n",
    "* When you are calling the LLM with several different input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arrr, AI Accelera has trained more than 5,000 Enterprise Alumni.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 5,000 Enterprise Alumni.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = ( \n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\")\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"How many enterprise Alumni has trained AI Accelera?\", \"language\": \"Pirate English\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableBranch: Router Chain\n",
    "* A RunnableBranch is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input.\n",
    "* **A RunnableBranch is initialized with a list of (condition, runnable) pairs and a default runnable**. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n",
    "* For advanced uses, a [custom function](https://python.langchain.com/v0.1/docs/expression_language/how_to/routing/) may be a better alternative than RunnableBranch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following advanced example can classify and respond to user questions based on specific topics like rock, politics, history, sports, or general inquiries. **It uses some new topics that we will explain in the following lesson**. Here’s a simplified explanation of each part:\n",
    "\n",
    "1. **Prompt Templates**: Each template is tailored for a specific topic:\n",
    "   - **rock_template**: Configured for rock and roll related questions.\n",
    "   - **politics_template**: Tailored to answer questions about politics.\n",
    "   - **history_template**: Designed for queries related to history.\n",
    "   - **sports_template**: Set up to address sports-related questions.\n",
    "   - **general_prompt**: A general template for queries that don't fit the specific categories.\n",
    "\n",
    "   Each template includes a placeholder `{input}` where the actual user question will be inserted.\n",
    "\n",
    "2. **RunnableBranch**: This is a branching mechanism that selects which template to use based on the topic of the question. It evaluates conditions (like `x[\"topic\"] == \"rock\"`) to determine the topic and uses the appropriate prompt template.\n",
    "\n",
    "3. **Topic Classifier**: A Pydantic class that classifies the topic of a user's question into one of the predefined categories (rock, politics, history, sports, or general).\n",
    "\n",
    "4. **Classifier Chain**:\n",
    "   - **Chain**: Processes the user's input to predict the topic.\n",
    "   - **Parser**: Extracts the predicted topic from the classifier's output.\n",
    "\n",
    "5. **RunnablePassthrough**: This component feeds the user's input and the classified topic into the RunnableBranch.\n",
    "\n",
    "6. **Final Chain**:\n",
    "   - The user's input is first processed to classify its topic.\n",
    "   - The appropriate prompt is then selected based on the classified topic.\n",
    "   - The selected prompt is used to formulate a question which is then sent to a model (like ChatOpenAI).\n",
    "   - The model’s response is parsed as a string and returned.\n",
    "\n",
    "7. **Execution**:\n",
    "   - The chain is invoked with a sample question, \"Who was Napoleon Bonaparte?\" \n",
    "   - Based on the classification, it selects the appropriate template, generates a query to the chat model, and processes the response.\n",
    "\n",
    "The system effectively creates a dynamic response generator that adjusts the way it answers based on the topic of the inquiry, making use of specialized knowledge for different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "rock_template = \"\"\"You are a very smart rock and roll professor. \\\n",
    "You are great at answering questions about rock and roll in a concise\\\n",
    "and easy to understand manner.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "rock_prompt = PromptTemplate.from_template(rock_template)\n",
    "\n",
    "politics_template = \"\"\"You are a very good politics professor. \\\n",
    "You are great at answering politics questions..\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "politics_prompt = PromptTemplate.from_template(politics_template)\n",
    "\n",
    "history_template = \"\"\"You are a very good history teacher. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_prompt = PromptTemplate.from_template(history_template)\n",
    "\n",
    "sports_template = \"\"\" You are a sports teacher.\\\n",
    "You are great at answering sports questions.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "sports_prompt = PromptTemplate.from_template(sports_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}\"\n",
    ")\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"rock\", rock_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"politics\", politics_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"history\", history_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"sports\", sports_prompt),\n",
    "  general_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"Classify the topic of the user question\"\n",
    "    \n",
    "    topic: Literal[\"rock\", \"politics\", \"history\", \"sports\"]\n",
    "    \"The topic of the user question. One of 'rock', 'politics', 'history', 'sports' or 'general'.\"\n",
    "\n",
    "classifier_function = convert_to_openai_function(TopicClassifier)\n",
    "\n",
    "llm = ChatOpenAI().bind(functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}) \n",
    "\n",
    "parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "\n",
    "classifier_chain = llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `classifier_function` classifies or categorizes the topic of a user's question into specific categories such as \"rock,\" \"politics,\" \"history,\" or \"sports.\" Here’s how it works in simple terms:\n",
    "\n",
    "1. **Conversion to Function**: It converts the `TopicClassifier` Pydantic class, which is a predefined classification system, into a function that can be easily used with LangChain. This conversion process involves wrapping the class so that it can be integrated and executed within an OpenAI model.\n",
    "\n",
    "2. **Topic Detection**: When you input a question, this function analyzes the content of the question to determine which category or topic it belongs to. It looks for keywords or patterns that match specific topics. For example, if the question is about a rock band, the classifier would identify the topic as \"rock.\"\n",
    "\n",
    "3. **Output**: The function outputs the identified topic as a simple label, like \"rock\" or \"history.\" This label is then used by other parts of the LangChain to decide how to handle the question, such as choosing the right template for formulating a response.\n",
    "\n",
    "In essence, the `classifier_function` acts as a smart filter that helps the system understand what kind of question is being asked so that it can respond more accurately and relevantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "final_chain = (\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"input\") | classifier_chain) \n",
    "    | prompt_branch \n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_chain.invoke(\n",
    "    {\"input\": \"Who was Napoleon Bonaparte?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in functions for Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of .bind() to add argumetns to a Runnable in a LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me a curious fact about {soccer_player}\")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about Cristiano '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model.bind(stop=[\"Ronaldo\"]) | output_parser\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
