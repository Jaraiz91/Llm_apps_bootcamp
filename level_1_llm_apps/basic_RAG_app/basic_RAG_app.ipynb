{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a simple RAG LLM App with LangChain\n",
    "A Retrieval Augmented Generation (RAG) app is a combination of searching for information and generating new content, all rolled into one application. It's like a smart assistant that not only finds the information you need but also uses that information to create new, useful responses. Here’s how it works, explained in simple terms:\n",
    "\n",
    "#### How a RAG App Works\n",
    "1. **Question or Query**: You start by asking the RAG app a question or giving it a topic you need information about.\n",
    "2. **Retrieval**: The app uses a **retriever** to go through a large document to find related information. This could be articles, books, or other relevant documents. Think of this step like finding all the ingredients you need for a recipe.\n",
    "3. **Augmented Generation**: Once the app has the information, it uses an LLM to generate the response. This generator takes all the information the retriever found and uses it to craft a detailed, accurate answer or content that directly addresses your question or topic.\n",
    "4. **Output**: Finally, the app presents you with a response that isn’t just copied from somewhere else but is a freshly generated piece of content based on the information it retrieved.\n",
    "\n",
    "#### Why It’s Useful\n",
    "- **Accuracy and Relevance**: Because the RAG app pulls information from relevant sources before generating a response, the answers you get are usually more accurate and relevant.\n",
    "- **Efficiency**: It combines the steps of searching for information and creating content, making it a powerful tool for quickly producing high-quality responses.\n",
    "- **Versatility**: This type of app can be used for a wide range of applications, from helping students with their homework to assisting researchers in summarizing scientific articles.\n",
    "\n",
    "#### RAG apps save you money\n",
    "Retrieval Augmented Generation (RAG) apps are particularly good for saving costs associated with large language models (LLMs).\n",
    "\n",
    "1. **Efficient Use of Resources**:\n",
    "   - **Selective Querying**: RAG apps first use a retriever to find relevant information before using a large language model to generate a response. This means the LLM is only called upon when necessary, after the relevant information has already been located. Instead of the LLM processing every aspect of a query from scratch, it focuses on integrating and enhancing the retrieved information to formulate a response.\n",
    "   - **Reduced Computational Load**: By leveraging retrieved content, the LLM performs less computation per query. This is because part of the work (finding relevant information) is already done, so the model just needs to synthesize this information rather than generating content from zero context, which is more resource-intensive.\n",
    "\n",
    "In essence, RAG apps help save on LLM costs by making sure that the expensive resources (like computing power and time) associated with running these large models are used judiciously and only where they provide the most value, such as in the generation of precise and contextually informed responses, rather than in the preliminary gathering of information.\n",
    "\n",
    "#### RAG apps and the privacy of your data\n",
    "RAG apps involve two main steps: retrieving information from documents and then using that information to generate responses. Whether these apps maintain the privacy of the documents they access largely depends on how they are designed and what data they handle. Here's a simple breakdown:\n",
    "\n",
    "#### RAG Apps and Data Privacy\n",
    "RAG is commonly deployed to solve large language model shortcomings, including hallucinations and short context windows. But RAG also helps us build privacy-aware apps.\n",
    "\n",
    "When using RAG, data is provided as context to an LLM only at generation time, but the data does not need to be used for training AI models. This means your data is not stored in the models themselves as knowledge — it’s only shown to LLMs when we ask for responses.\n",
    "\n",
    "Instead of storing your data permanently in the AI model, RAG uses the data only when needed to generate a response. After the response is generated, the data isn't stored or remembered by the LLM.\n",
    "\n",
    "In a production-level RAG Application, successfully building privacy-aware solution requires considering and classifying the data you plan to store upfront, and taking advanced steps to keep your sensible data safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('./data/be-good.txt')\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the idea of creating something people want and not worrying about making money initially. It suggests that embodying this concept can lead to success, using examples like Craigslist and Octopart. Additionally, it explores the benefits of doing good and how it can attract support and aid in overcoming challenges.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=['context', 'question'],\n",
    "    metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, \n",
    "    messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], \n",
    "                                                               template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke('what is the article about?')\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
