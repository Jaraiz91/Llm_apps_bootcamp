{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader, UnstructuredExcelLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from operator import itemgetter\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm bootcamp\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "print(os.environ['LANGCHAIN_PROJECT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hola! ¿Cómo estás?', additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2025-01-07T16:53:44.5058933Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 4508955400, 'load_duration': 2820797700, 'prompt_eval_count': 12, 'prompt_eval_duration': 824117000, 'eval_count': 8, 'eval_duration': 861864000}, id='run-d7b29b11-c81e-4696-8798-773ea95fcaf4-0', usage_metadata={'input_tokens': 12, 'output_tokens': 8, 'total_tokens': 20})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model='llama3')\n",
    "llm.invoke('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "splitters = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_loader = UnstructuredExcelLoader('./data/ventas_farmacia_diciembre.xlsx')\n",
    "excel_docs = sales_loader.load()\n",
    "text_loader = TextLoader('./data/farmacia.txt')\n",
    "text_docs = text_loader.load()\n",
    "pharma_docs = [excel_docs, text_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_docs = []\n",
    "for d in pharma_docs:\n",
    "    splitted = splitters.split_documents(d)\n",
    "    splitted_docs.append(splitted)\n",
    "docs = [x for sublist in splitted_docs for x  in (sublist if isinstance(sublist, list)else [sublist]) ]\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_message = \"\"\"Eres un asistente de una farmacia. Tu trabajo consiste en proporcionar respuestas a las preguntas que se te hacen teniendo en cuenta los documentos proporcionados.\n",
    "Importante solo responder a las preguntas que estén relacionadas con el contexto. Si no sabes la respuesta no te la inventes. Utiliza el contexto para responder a la pregunta {question}\n",
    "\n",
    "contexto: {context}\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {'context':  retriever | format_docs, 'question': RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La farmacia cuenta con un equipo de 6 empleados."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream('cuantos empleados tiene la farmacia?'):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Según el contexto, los productos que se han vendido en diciembre son:\n",
      "\n",
      "* Ibuprofeno: 15 + 17 + 14 + 4 + 1 + 17 + 19 + 8 = 112 unidades\n",
      "* Lorazepam: 6 + 12 + 19 + 17 + 10 + 14 + 1 + 13 = 102 unidades\n",
      "* Minoxidil: 11 + 7 + 3 + 2 + 1 + 15 + 19 + 3 = 73 unidades\n",
      "* Almax: 17 + 14 + 17 + 3 + 1 + 18 + 10 + 1 = 81 unidades"
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream('que productos se han vendido en diciembre?'):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Según el contexto proporcionado, no hay información específica sobre las ventas de productos en diciembre ni sobre el producto que más se vendió ese mes. Por lo tanto, no puedo responder a esa pregunta. Si necesitas saber algo relacionado con los productos o servicios ofrecidos por la farmacia, estoy aquí para ayudarte.'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({'question':'cual es el producto que más se ha vendido en diciembre?', 'history': 'hi'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las funciones de un auxiliar de farmacia en Farmacia Miranda Sanz serían brindar asesoramiento y atención en la salud y bienestar de la comunidad, junto a los otros 2 auxiliares y los 3 farmacéuticos que componen el equipo de empleados.\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "rag_memory_message = \"\"\"Eres un asistente de una farmacia. Tu trabajo consiste en proporcionar respuestas a las preguntas que se te hacen teniendo en cuenta los documentos proporcionados.\n",
    "Importante solo responder a las preguntas que estén relacionadas con el contexto. Si no sabes la respuesta no te la inventes. Utiliza el contexto y el historial de la conversación para responder a la pregunta {question}\n",
    "\n",
    "contexto: {context}\n",
    "historial:{chat_history}\"\"\"\n",
    "rag_memory_prompt = ChatPromptTemplate.from_template(rag_memory_message)\n",
    "chat_history = memory.load_memory_variables({})\n",
    "\n",
    "memory_chain = ({'context': itemgetter('question') | retriever | format_docs, 'question': itemgetter('question'), 'chat_history': itemgetter('history')}\n",
    "                | rag_memory_prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    ")\n",
    "# Datos de entrada\n",
    "input_data = {\n",
    "    'question': '¿Cuáles son las funciones de un auxiliar de farmacia?',\n",
    "    'history': chat_history\n",
    "}\n",
    "\n",
    "# Ejecutar la cadena\n",
    "response = memory_chain.invoke(input_data)\n",
    "print(response)\n",
    "\n",
    "# Guardar la nueva interacción en la memoria\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "HumanMessage(input_data['question'])\n",
    "inputs = {'question': input_data['question']}\n",
    "outputs = {'response': response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='¿Cuáles son las funciones de un auxiliar de farmacia?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Las funciones de un auxiliar de farmacia en Farmacia Miranda Sanz serían brindar asesoramiento y atención en la salud y bienestar de la comunidad, junto a los otros 2 auxiliares y los 3 farmacéuticos que componen el equipo de empleados.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = memory.load_memory_variables({})\n",
    "chat_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
