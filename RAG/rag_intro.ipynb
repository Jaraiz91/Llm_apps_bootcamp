{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 of indexing: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(dotenv_path='./RAG/.env')\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "groq_api_key = os.environ['GROQ_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llm bootcamp'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional info on Loading Data\n",
    "* [Documentation on Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/). Interesting points:\n",
    "    * PDF. Check the Multimodal LLM Apps section.\n",
    "    * CSV.\n",
    "    * JSON.\n",
    "    * HTML.\n",
    "* [Integrations with 3rd Party Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/). Interesting points:\n",
    "    * Email.\n",
    "    * Github.\n",
    "    * Google Drive.\n",
    "    * HugggingFace dataset.\n",
    "    * Images.\n",
    "    * Jupyter Notebook.\n",
    "    * MS Excel.\n",
    "    * MS Powerpoint.\n",
    "    * MS Word.\n",
    "    * MongoDB.\n",
    "    * Pandas DataFrame.\n",
    "    * Twitter.\n",
    "    * URL.\n",
    "    * Unstructured file.\n",
    "    * WebBaseLoader.\n",
    "    * YouTube audio.\n",
    "    * YouTube transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Split Data into Small Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Step: Transform the small chunks into embeddings and store them into a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional info on Embeddings\n",
    "* [Documentation on Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding).\n",
    "* [Integration with 3rd Party Embedding Models](https://python.langchain.com/docs/integrations/text_embedding/). Interesting points:\n",
    "    *  OpenAI.\n",
    "    *  Cohere.\n",
    "    *  Fireworks.\n",
    "    *  Google Vertex AI.\n",
    "    *  Hugging Face.\n",
    "    *  Llama-ccp.\n",
    "    *  MistralAI.\n",
    "    *  Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional info on Vector Databases (also called Vector Stores)\n",
    "* [Documentation on Vector Databases](https://python.langchain.com/docs/modules/data_connection/vectorstores/).\n",
    "* [Integration with 3rd Party Vector Databases](https://python.langchain.com/docs/integrations/vectorstores/). Interesting points:\n",
    "    * Chroma.\n",
    "    * Faiss.\n",
    "    * Postgres.\n",
    "    * PGVector.\n",
    "    * Databricks.\n",
    "    * Pinecone.\n",
    "    * Redis.\n",
    "    * Supabase.\n",
    "    * LanceDB.\n",
    "    * MongoDB.\n",
    "    * Neo4j.\n",
    "    * Activeloop DeepLake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Retrieval\n",
    "* We are going to create a RAG app that\n",
    "    * takes a user question,\n",
    "    * searches for documents relevant to that question,\n",
    "    * passes the retrieved documents and initial question to a model,\n",
    "    * and returns an answer.\n",
    "* First we need to define our logic for searching over documents. LangChain defines a Retriever interface which wraps an index that can return relevant Documents given a string query.\n",
    "* The most common type of Retriever is the VectorStoreRetriever, which uses the similarity search capabilities of a vector store to facilitate retrieval. Any VectorStore can easily be turned into a Retriever with VectorStore.as_retriever()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the retriever\n",
    "A \"retriever\" refers to a component or object that is used to retrieve (or search for) specific information from a database or a collection of data. The retriever is designed to work with a vector database that stores data in the form of embeddings.\n",
    "\n",
    "Embeddings are numerical representations of data (in this case, text data split into smaller chunks) that capture the semantic meaning of the original content. These embeddings are created using models like the ones provided by OpenAI (for example, through `OpenAIEmbeddings`), which convert textual content into high-dimensional vectors that numerically represent the semantic content of the text.\n",
    "\n",
    "The vector database, in this context represented by `Chroma`, is a specialized database that stores these embeddings. The primary function of this database is to enable efficient similarity searches. That is, given a query in the form of an embedding, the database can quickly find and return the most similar embeddings (and by extension, the corresponding chunks of text or documents) stored within it.\n",
    "\n",
    "When the code sets the `retriever` as `vectorstore.as_retriever()`, it essentially initializes a retriever object with the capability to perform these similarity-based searches within the `Chroma` vector database. This means that the retriever can be used to find the most relevant pieces of information (text chunks, in this scenario) based on a query. This is particularly useful in applications that needs to retrieve information based on semantic similarity rather than exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional information on Retrievers.\n",
    "* [Documentation on Retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers/). Interesting points:\n",
    "    * Vectorstore.\n",
    "    * Time-Weighted Vectorstore: most recent first.\n",
    "    * Advanced Retrievers. \n",
    "* [Integrations with 3rd Party Retrieval Services](https://python.langchain.com/docs/integrations/retrievers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()  }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the LCEL Runnable protocol to define the chain allows us to:\n",
    "    * pipe together components and functions in a transparent way.\n",
    "    * automatically trace our chain in LangSmith.\n",
    "    * get streaming, async, and batched calling out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables\n",
    "A \"Runnable\" is like a tool or a small machine in a factory line that has a specific job. You can think of it as a mini-program that knows how to do one particular task, like read data, change it, or make decisions based on that data.\n",
    "\n",
    "A \"Runnable\" in the context of LangChain is essentially a component or a piece of code that can be executed or \"run\". \n",
    "\n",
    "It acts like a building block that can perform a specific operation or process data in a particular way.\n",
    "\n",
    "The chaining or composition of Runnables allows you to build a sequence where data flows from one Runnable to another, each performing its function, ultimately leading to a final result.\n",
    "\n",
    "In essence, a Runnable makes it easy to define and reuse modular operations that can be combined in various ways to build complex data processing and interaction pipelines efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RunnableParallel\n",
    "RunnableParallel is like a manager that can handle multiple tasks at once in a software system. Instead of doing one job at a time, it can do several jobs simultaneously, making the process faster and more efficient.\n",
    "\n",
    "RunnableParallel is used to manage different components that do tasks like retrieving information, passing questions, or formatting data, and then brings together all their outputs neatly. This makes it very handy for building complex operations where multiple, independent processes need to run at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RunnablePassThrough\n",
    "RunnablePassthrough is a simple tool that lets data pass through it without making any changes. You can think of it like a clear tube: whatever you send into one end comes out the other end unchanged.\n",
    "\n",
    "When working with data transformations and pipelines, RunnablePassthrough is often used alongside other tools that might modify the data. For example, when multiple operations are run in parallel, you might want to let some data flow through unaltered while other data gets modified or processed differently.\n",
    "\n",
    "RunnablePassthrough is often used in combination with RunnableParallel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can start asking questions to our RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will use streaming, so the answer will flow like what we see using chatGPT instead printing the whole answer in one single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is a technique that breaks down complex tasks into smaller and simpler steps. It helps agents manage and interpret the thinking process by transforming big tasks into manageable ones. This process can be done through prompting techniques like Chain of Thought or Tree of Thoughts."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative approach with a customized prompt and without streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps, making them more manageable for agents or models to handle. It can be achieved through methods like Chain of Thought or Tree of Thoughts, which explore multiple reasoning possibilities at each step. Thanks for asking!'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Information on Chat Models and LLM Models\n",
    "* [Documentation on Chat Models](https://python.langchain.com/docs/modules/model_io/chat/). Interesting points:\n",
    "    * How-To Guides.\n",
    "    * Function calling.\n",
    "    * Streaming. \n",
    "* [Integration with 3rd Party Chat Models](https://python.langchain.com/docs/integrations/chat/). Interesting points:\n",
    "    * OpenAI.\n",
    "    * Anthropic.\n",
    "    * Azure OpenAI.\n",
    "    * Cohere.\n",
    "    * Fireworks.\n",
    "    * Google AI.\n",
    "    * Hugging Face.\n",
    "    * Llama 2 Chat.\n",
    "    * MistralAI.\n",
    "    * Ollama.\n",
    "* [Documentation on LLM Models](https://python.langchain.com/docs/modules/model_io/llms). Interesting points:\n",
    "    * How-To Guides.\n",
    "    * Streaming.\n",
    "    * Tracking token usage. \n",
    "* [Integrations with 3rd Party LLM Models](https://python.langchain.com/docs/integrations/llms). Interesting points:\n",
    "    * OpenAI.\n",
    "    * Anthropic.\n",
    "    * Azure OpenAI.\n",
    "    * Cohere.\n",
    "    * Fireworks.\n",
    "    * Google AI.\n",
    "    * GPT4All.\n",
    "    * Hugging Face.\n",
    "    * Llama.cpp\n",
    "    * Ollama.\n",
    "    * Replicate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
